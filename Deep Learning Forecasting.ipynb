{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d82ecf85",
   "metadata": {},
   "source": [
    "# Machine Learning Quick Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359176c9",
   "metadata": {},
   "source": [
    "## Evaluation metrics\n",
    "\n",
    "In general, we want to compare how close are the predictions to the actual numbers in the test set.\n",
    "\n",
    "This is typically assessed using:\n",
    "\n",
    "- MSE for quantitative response\n",
    "- Misclassification rate for qualitative response\n",
    "\n",
    "![](Pictures/metrics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d328efaa",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63864c78",
   "metadata": {},
   "source": [
    "The data set is typically divided into 3 non-overlapping samples:\n",
    "\n",
    "- Training set: to train the model\n",
    "- Validation set: to validate and tune the model (hyperparameters tuning)\n",
    "- Test set: to test the model's ability to predict well on new data (generalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e5c1e6",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "- Learning: Finding the model weights (parameters' values)\n",
    "- Cost functions: Tells us how good our model is at making prediction for a given set of parameters.\n",
    "- The cost function has its own curve and its own gradients. The slope of this curve tells us how to update our parameters to make the model more accurate.\n",
    "- The two most frequently used optimization algroithms when the cost function is continuous and differentiable are Gradient Descent (GD) and Stochastic GD.\n",
    "\n",
    "GD is an iterative optimization algorithm for finding the minimum of a function.\n",
    "\n",
    "We starts at some random point and take steps proportional to the negative of the gradient of the function at the current point.\n",
    "\n",
    "$$\\theta_{j} = \\theta_{j} - \\alpha \\frac{\\partial}{\\partial \\theta_j} L(\\theta)\\$$\n",
    "\n",
    "where $\\theta_j$ is the model's jth parameter, $\\alpha$ is the learning rate, and $L(\\theta)$ is the cost function (differentiable).\n",
    "\n",
    "Drawbacks of GD:\n",
    "\n",
    "1. Single batch: use the entire training set to update paramters\n",
    "2. Sensitive to learning rate\n",
    "3. Slow for large datasets\n",
    "\n",
    "(Minibatch) SGD: is a version of the algorithm that speeds up the computation by approximating the gradient using smaller batches (subsets) of the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06614eb2",
   "metadata": {},
   "source": [
    "# A brief introduction to Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689d4e84",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "Forward propagation is the process of calculating the ourput of a neural network, given an input.\n",
    "\n",
    "\n",
    "## Neurons\n",
    "\n",
    "Neurons are the simplest elements or building blocks in a neural network. They are inspired by biological neurons that are found in the human brain.\n",
    "\n",
    "## Activation function\n",
    "\n",
    "An activation function is a mathematical function that is applied to the output of each neuron in a neural network.\n",
    "\n",
    "Activation functions allow the network to learn non-linearity and complex patterns from the data and make accurate predictions.\n",
    "\n",
    "\n",
    "![](Pictures/activation_fcn.jpeg)\n",
    "\n",
    "### Softmax Activation function\n",
    "\n",
    "Softmax activation function is often used in classification tasks, where the goal is to predict which of a fixed set of classes (more than two classes) a particular sample belongs to.\n",
    "\n",
    "The Softmax function takes in a vector of real numbers and converts it into a probability distribution, where the sum of all the probabilities is equal to 1.\n",
    "\n",
    "![](Pictures/softmax.png)\n",
    "\n",
    "\n",
    "### Which activation function?\n",
    "\n",
    "Again, there is no ‚Äúright‚Äù answer! However:\n",
    "- For the hidden layers, almost always tanh is better than sigmoid because it center the data for the next layer.\n",
    "- One downside of both sigmoid or tanh is that the gradient is very small for extreme values.\n",
    "\n",
    "- In practice, RELU works better/faster than sigmoid or tanh for hidden layers.\n",
    "- Leaky RELU might work better than RELU, but if we have enough number of hidden layers, RELU is just\n",
    "fine.\n",
    "- Sigmoid is mostly used for output layer!\n",
    "\n",
    "![](Pictures/common_activation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f3c977",
   "metadata": {},
   "source": [
    "## How to define good functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a677ef",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Loss functions quantifies the disparity between the predicted output of the model and the actual target values. The loss function generates a single scalar value that represents the error, and the objective during training is to minimize this error (lower is better).\n",
    "\n",
    "Common loss functions:\n",
    "\n",
    "- Square loss : (Mean Squared Error for regression):\n",
    "\n",
    "    This is commonly used for regression tasks.\n",
    "\n",
    "- Hinge loss:  (for binary classification)\n",
    "\n",
    "    This is often used with Support Vector Machines (SVMs) and is suitable for binary classification problems.\n",
    "\n",
    "- Logistic loss:(Binary Cross Entropy Loss for binary classification)\n",
    "\n",
    "    This is commonly used in logistic regression and binary classification tasks.\n",
    "\n",
    "- Cross entropy loss loss:  (Categorical Cross Entropy for multiclass classification)\n",
    "\n",
    "    This is often used for multiclass classification problems.\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb0a1f8",
   "metadata": {},
   "source": [
    "## Evaluating a time series model\n",
    "\n",
    "The main thing we will be evaluating is: **how do our model's predictions (`y_pred`) compare against the actual values (`y_true` or *ground truth values*)**?\n",
    "\n",
    "### Scale-dependent errors\n",
    "\n",
    "These are metrics which can be used to compare time series values and forecasts that are on the same scale.\n",
    "\n",
    "\n",
    "| Metric | Details | Code |\n",
    "| ----- | ----- | ----- |\n",
    "| **MAE** (mean absolute error) | Easy to interpret (a forecast is X amount different from actual amount). Forecast methods which minimises the MAE will lead to forecasts of the median. | [`tf.keras.metrics.mean_absolute_error()`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanAbsoluteError) |\n",
    "| **RMSE** (root mean square error) | Forecasts which minimise the RMSE lead to forecasts of the mean. | `tf.sqrt(`[`tf.keras.metrics.mean_square_error()`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/RootMeanSquaredError)`)`|\n",
    "\n",
    "### Percentage errors\n",
    "\n",
    "Percentage errors do not have units, this means they can be used to compare forecasts across different datasets.\n",
    "\n",
    "| **Metric** | **Details** | **Code** |\n",
    "| ----- | ----- | ----- |\n",
    "| **MAPE** (mean absolute percentage error) | Most commonly used percentage error. May explode (not work) if `y=0`. | [`tf.keras.metrics.mean_absolute_percentage_error()`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MAPE) |\n",
    "| **sMAPE** (symmetric mean absolute percentage error) | Recommended not to be used by [Forecasting: Principles and Practice](https://otexts.com/fpp3/accuracy.html#percentage-errors), though it is used in forecasting competitions. | Custom implementation |\n",
    "\n",
    "### Scaled errors\n",
    "\n",
    "Scaled errors are an alternative to percentage errors when comparing forecast performance across different time series.\n",
    "\n",
    "| **Metric** | **Details** | **Code** |\n",
    "| ----- | ----- | ----- |\n",
    "| **MASE** (mean absolute scaled error). | MASE equals one for the naive forecast (or very close to one). A forecast which performs better than the na√Øve should get <1 MASE. | See sktime's [`mase_loss()`](https://github.com/alan-turing-institute/sktime/blob/ee7a06843a44f4aaec7582d847e36073a9ab0566/sktime/performance_metrics/forecasting/_functions.py#L16) |\n",
    "\n",
    "> ü§î **Question:** There are so many metrics... which one should I pay most attention to? It's going to depend on your problem. However, since its ease of interpretation (you can explain it in a sentence to your grandma), MAE is often a very good place to start.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfaafbc",
   "metadata": {},
   "source": [
    "## Building a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29bde0f",
   "metadata": {},
   "source": [
    "A neural network is a type of machine learning model that is composed of layers of interconnected \"neurons\" which process and transmit information.\n",
    "\n",
    "Each neuron receives input from other neurons, processes it using a nonlinear activation function, and then transmits the output to other neurons in the next layer. The output of the final layer is the prediction made by the neural network.\n",
    "\n",
    "Because all the inputs are densely connected to all outputs, these layers are called Dense layers. \n",
    "\n",
    "The output layer can be either single output or multiple output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c4a47",
   "metadata": {},
   "source": [
    "## Building a Deep Neural Network\n",
    "\n",
    "Deep neural networks are neural networks with a large number of layers, typically consisting of multiple hidden layers.\n",
    "\n",
    "Deep neural networks can learn and model very complex patterns in data.\n",
    "\n",
    "DNNs have been successful in a wide range of applications, including image and speech recognition, natural language processing, and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a00df47",
   "metadata": {},
   "source": [
    "## Train a Neural Network: BackPropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38303ab9",
   "metadata": {},
   "source": [
    "To train a neural network, we present it with many examples and adjust the weights and biases of the connections between neurons so that the network can accurately predict the output for each example and minimize the loss function.\n",
    "\n",
    "- This process is known as backpropagation, and it is done using an optimization algorithm such as stochastic gradient descent.\n",
    "- The loss function quantifies the distance between actuals and predictions. It provides feedback to the NN.\n",
    "- For the full list of loss functions, visit https://keras.io/api/losses/#available-losses\n",
    "\n",
    "Backpropagation is a supervised learning algorithm that adjusts the weights and biases of the connections between neurons in the network to minimize the loss function.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Feed the input data through the neural network to compute the output.\n",
    "2. Calculate the loss or error between the predicted output and the true output.\n",
    "3. Propagate the error back through the network using the chain rule of calculus to calculate the gradient of the loss function with respect to the weights and biases.\n",
    "4. Update the weights and biases using the gradient descent algorithm.\n",
    "5. Repeat the process until the loss reaches a satisfactory level or a predetermined iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56149cf2",
   "metadata": {},
   "source": [
    "## Handle Overfiting in DNN\n",
    "\n",
    "Regularization refers to a set of techniques that are used to prevent overfitting by discouraging complex models.\n",
    "\n",
    "Regularization improve generalization of our model on unseen data.\n",
    "\n",
    "![](Pictures/Regularization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c1d439",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f901477",
   "metadata": {},
   "source": [
    "### Batch size and Epoch\n",
    "\n",
    "The batch size is a hyperparameter that defines the number of samples to work through before updating the internal model parameters.\n",
    "\n",
    "A training dataset can be divided into one or more batches.\n",
    "\n",
    "- Batch Gradient Descent. Batch Size = Size of Training Set\n",
    "- Stochastic Gradient Descent. Batch Size = 1\n",
    "- Mini-Batch Gradient Descent. 1 < Batch Size < Size of Training Set\n",
    "\n",
    "In the case of mini-batch gradient descent, popular batch sizes include 32, 64, and 128 samples. You may see these values used in models in the literature and in tutorials.\n",
    "\n",
    "The number of epochs is a hyperparameter that defines the number times that the learning algorithm will work through the entire training dataset.\n",
    "\n",
    "One epoch means that each sample in the training dataset has had an opportunity to update the internal model parameters. An epoch is comprised of one or more batches. For example, as above, an epoch that has one batch is called the batch gradient descent learning algorithm.\n",
    "\n",
    "In a nutshell:\n",
    "\n",
    "- The batch size is a number of samples processed before the model is updated.\n",
    "\n",
    "- The number of epochs is the number of complete passes through the training dataset.\n",
    "\n",
    "![](Pictures/batchsize.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa312aa",
   "metadata": {},
   "source": [
    "### Adaptive Learning Rate and Momentum: to adapt to the loss landscape\n",
    "\n",
    "Rather than just looking at the current gradient, consider the previous weight update. This is called momentum \n",
    "\n",
    "**Âü∫Êú¨ÂéüÂâáÔºö**\n",
    "\n",
    "- Êüê‰∏ÄÂÄãÊñπÂêë‰∏ä gradient ÁöÑÂÄºÂæàÂ∞èÔºåÈùûÂ∏∏ÁöÑÂπ≥Âù¶ ‚áí learning rate Ë™øÂ§ß‰∏ÄÈªû\n",
    "- Êüê‰∏ÄÂÄãÊñπÂêë‰∏äÈùûÂ∏∏ÁöÑÈô°Â≥≠ÔºåÂù°Â∫¶ÂæàÂ§ß ‚áí learning rate ÂèØ‰ª•Ë®≠Â∞è‰∏ÄÈªû\n",
    "\n",
    "Examples:\n",
    "\n",
    "1. Adagrad:ËÄÉÊÖÆ‰πãÂâçÊâÄÊúâÁöÑÊ¢ØÂ∫¶Â§ßÂ∞è\n",
    "\n",
    "2. RMSProp Ë™øÊï¥Áï∂ÂâçÊ¢ØÂ∫¶ËàáÈÅéÂéªÊ¢ØÂ∫¶ÁöÑÈáçË¶ÅÊÄß\n",
    "\n",
    "3. Adam = RMSProp + MomentumÔºàÊúÄÂ∏∏Áî®ÁöÑÁ≠ñÁï•Ôºâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee0a46f",
   "metadata": {},
   "source": [
    "There are many different ways to potentially improve a neural network. Some of the most common include: \n",
    "\n",
    "- increasing the number of layers (making the network deeper), \n",
    "- increasing the number of hidden units (making the network wider) and \n",
    "- changing the learning rate. \n",
    "\n",
    "Because these values are all human-changeable, they're referred to as hyperparameters) and the practice of trying to find the best hyperparameters is referred to as hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adf2f82",
   "metadata": {},
   "source": [
    "# MLP or FCN\n",
    "\n",
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9924aaa",
   "metadata": {},
   "source": [
    "# RNN Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099e99f0",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c310461",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks are a family of networks for processing sequential data (time series, text, audio, video)\n",
    "\n",
    "\n",
    "RNN can be used for a number of sequence-based problems:\n",
    "\n",
    "- One to one: one input, one output, such as image classification.\n",
    "- One to many: one input, many outputs, such as image captioning (image input, a sequence of text as caption output).\n",
    "- Many to one: many inputs, one outputs, such as text classification (classifying a Tweet as real diaster or not real diaster).\n",
    "- Many to many: many inputs, many outputs, such as machine translation (translating English to Spanish) or speech to text (audio wave as input, text as output).\n",
    "\n",
    "![Input/Output sequence](Pictures/IO.png)\n",
    "\n",
    "\n",
    "RNN have memory: prior input influence the current input and output  ‚Üí  output depends on prior elements:\n",
    "\n",
    "How can RNN do this? Ans: It applys a recurrence relation at every time step to process a sequence\n",
    "\n",
    "\\begin{align}\n",
    "h_t = f_W(h_{t-1},x_t)\n",
    "\\end{align}\n",
    "\n",
    "where $h_t$ is cell state, $f_W$ is a function parameterized by $W$, $h_{t-1}$ is old state, and $x_t$ is a input vector at time step t.\n",
    "\n",
    "\n",
    "![RNN have loops](./Pictures/RNN_schematics.png)\n",
    "\n",
    "\n",
    "In the above diagram, a chunk of neural network (purple chunk), looks at some input $x_t$ and outputs a value $h_t$. A loop allows information to be passed from one step of the network to the next.\n",
    "\n",
    "Once you unfold it, a recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor.\n",
    "\n",
    "![The repeating module in a standard RNN contains a single layer](./Pictures/LSTM3-SimpleRNN.png)\n",
    "\n",
    "In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.\n",
    "\n",
    "\n",
    "N.B.: The same function $f$ and set of parameters $\\theta=\\{U, V, W\\}$ are used at every time step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42324f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN intuition pseudo code\n",
    "\n",
    "my_rnn = RNN()\n",
    "\n",
    "hidden_state = [0,0,0,0]\n",
    "\n",
    "sentence = [\"I\", \"love\", \"RNN\"]\n",
    "\n",
    "for word in sentence:\n",
    "    prediction, hidden_state = my_rnn(word, hidden_state)\n",
    "    \n",
    "next_word_prediction = prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7617ce4b",
   "metadata": {},
   "source": [
    "At each time step, \n",
    "\n",
    "Given our input vector $x_t$, RNN applys a function to update its hidden state:\n",
    "\n",
    "$h_t=\\sigma(Wh_{t-1}+Ux_t) = \\sigma(W_{hh}h_{t-1}+W_{hx}x_t)$, where $\\sigma(\\cdot):$ activation functions\n",
    "\n",
    "$\\hat y_t=o_t=Vh_t = W_{yh}h_t$ \n",
    "\n",
    "Then, we can compute loss $L_t$ at each time step\n",
    "\n",
    "The total loss is simply the losses from all the individual loss at each time step, i.e., $L = \\sum_{t}L_t$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc9cc51",
   "metadata": {},
   "source": [
    "## Model Training Issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca50498",
   "metadata": {},
   "source": [
    "In RNN, losses are back propagated at each individual time step and finally across all time steps, all the way from the end of sequence to the begining. This is so-called **Back Propagation Through Time (BPTT)**.\n",
    "\n",
    "Backpropagation Through Time, or BPTT, is the training algorithm used to update weights in recurrent neural networks like LSTMs.\n",
    "\n",
    "Conceptually, BPTT works by unrolling all input timesteps. Each timestep has one input timestep, one copy of the network, and one output. Errors are then calculated and accumulated for each timestep. The network is rolled back up and the weights are updated.\n",
    "\n",
    "We can summarize the algorithm as follows:\n",
    "\n",
    "1. Present a sequence of timesteps of input and output pairs to the network.\n",
    "2. Unroll the network then calculate and accumulate errors across each timestep.\n",
    "3. Roll-up the network and update weights.\n",
    "4. Repeat.\n",
    "\n",
    "All model parameters $\\theta$ can be updated by \n",
    "\n",
    "\\begin{align}\n",
    "\\theta^{i+1}=\\theta^{i}-\\eta \\nabla_{\\theta}L(\\theta^i)\n",
    "\\end{align}\n",
    "\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "Let $L(\\theta)$ be the total loss function, since the total loss is simply the losses from all the individual loss at each time step, we have $L = L_0+L_1+...L_T$\n",
    "\n",
    "In particular, let's focus on the paramter $W$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L_0}{\\partial W}+\\frac{\\partial L_1}{\\partial W}+...+\\frac{\\partial L_T}{\\partial W}$\n",
    "\n",
    "$\\frac{\\partial L_0}{\\partial W} = \\frac{\\partial L_0}{\\partial y_0}\\frac{\\partial y_0}{\\partial h_0}\\frac{\\partial h_0}{\\partial W}$\n",
    "\n",
    "$\\frac{\\partial L_1}{\\partial W} = \\frac{\\partial L_1}{\\partial y_1}\\frac{\\partial y_1}{\\partial h_1}\\frac{\\partial h_1}{\\partial W} = \\frac{\\partial L_1}{\\partial y_1}\\frac{\\partial y_1}{\\partial h_1}\\frac{\\partial h_1}{\\partial h_0}\\frac{\\partial h_0}{\\partial W}$\n",
    "\n",
    "$\\vdots$\n",
    "\n",
    "$\\frac{\\partial L_T}{\\partial W} = \\sum_{k=0}^{T}\\frac{\\partial L_T}{\\partial y_T} \\frac{\\partial y_T}{\\partial h_T}\\frac{\\partial h_T}{\\partial h_k} \\frac{\\partial h_k}{\\partial W}$\n",
    "\n",
    "where $\\frac{\\partial h_T}{\\partial h_k}=\\prod_{j=k+1}^{T}\\frac{\\partial h_j}{\\partial h_{j-1}}$\n",
    "\n",
    "Notice that each partial is a Jacobian matrix, and thus the gradient is a product of Jacobian matrices, each associated with a step in the forward computation.\n",
    "\n",
    "As the time horizon gets bigger, this product gets longer and longer.\n",
    "If many of the values involved in these multiplications are smaller than 1, we are multiplying a lot of small numbers, which leads to small gradients and thus results in biased parameters and unable to capture long term dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d393dbe5",
   "metadata": {},
   "source": [
    "Computing the gradient w.r.t $h_0$ involves many factors of $W$ and repeated gradient computation. This is problematic!!\n",
    "\n",
    "Why? \n",
    "\n",
    "Ans: \n",
    "\n",
    "- The gradient is a product of Jacobian matrices, each associated with a step in the forward computation.\n",
    "\n",
    "- Multiply the **same** matrix at each tiem step during backpropation.\n",
    "\n",
    "Consider two cases:\n",
    "\n",
    "1. Exploding gradients: when there are many values involved in these computations are > 1. In this case, the gradient become extremely large, and we cannot optimize them.\n",
    "\n",
    "2. Vanishing gradients: when there are many values involved in these computations are < 1. In this case, the gradient become extremely small, and we cannot optimize them.\n",
    "\n",
    "Exploding gradients problem is relatively easy to solve (e.g. clipping); However, the Vanishing gradients problem is much troublesome (a popular solution is gating). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557e23d8",
   "metadata": {},
   "source": [
    "How to solve vanishing gradient problem?\n",
    "\n",
    "1. Use Activation Function that prevents fast shrinkage of gradient\n",
    "2. Use weight initialization techniques that ensure that the initial weights are not too small\n",
    "3. Use gradient clipping which limits the magnitude of the gradients from becoming too\n",
    "small (vanishing gradient) or too large (exploding gradient)\n",
    "4. Use batch normalization, which normalizes the input to each layer and helps to reduce the\n",
    "range of activation values and thus the likelihood of vanishing gradients.\n",
    "5. Use a different optimization algorithm that is more resilient to vanishing gradients, such\n",
    "as Adam or RMSprop.\n",
    "6. Gated cells: Use some sort of skip connections, which allow gradients to bypass some\n",
    "of the layers in the network and thus prevent them from becoming too small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783b4fef",
   "metadata": {},
   "source": [
    "## The Problem of Long-Term Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cead5d3",
   "metadata": {},
   "source": [
    "Sometimes, we only need to look at recent information to perform the present task. In such cases, where the gap between the relevant information and the place that it‚Äôs needed is small, RNNs can learn to use the past information.\n",
    "\n",
    "But there are also cases where we need more context.It‚Äôs entirely possible for the gap between the relevant information and the point where it is needed to become very large.\n",
    "\n",
    "Unfortunately, as that gap grows, RNNs become unable to learn to connect the information.\n",
    "\n",
    "In theory, RNNs are absolutely capable of handling such ‚Äúlong-term dependencies.‚Äù A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs don‚Äôt seem to be able to learn them. The problem was explored in depth by **Hochreiter (1991) [German]** and **Bengio, et al. (1994)**, who found some pretty fundamental reasons why it might be difficult.\n",
    "\n",
    "Thankfully, LSTMs don‚Äôt have this problem!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1795ad67",
   "metadata": {},
   "source": [
    "# LSTM Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba761b25",
   "metadata": {},
   "source": [
    "Long Short Term Memory networks ‚Äì usually just called ‚ÄúLSTMs‚Äù ‚Äì are a special kind of RNN, capable of learning long-term dependencies. They were introduced by **Hochreiter & Schmidhuber (1997)**, and were refined and popularized by many people in following work. They work tremendously well on a large variety of problems, and are now widely used.\n",
    "\n",
    "LSTMs are explicitly designed to avoid the long-term dependency problem (by **Gating**). Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!\n",
    "\n",
    "Before we get started, let's introuduce some notations first.\n",
    "\n",
    "![notations](Pictures/LSTM2-notation.png)\n",
    "\n",
    "In the above diagram, each line carries an entire vector, from the output of one node to the inputs of others. The pink circles represent pointwise operations, like vector addition, while the yellow boxes are learned neural network layers. Lines merging denote concatenation, while a line forking denote its content being copied and the copies going to different locations.\n",
    "\n",
    "![chain](Pictures/LSTM3-chain.png)\n",
    "\n",
    "Instead of having a single neural network layer like in RNN, there are four, interacting in a very special way that control information flow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d55698",
   "metadata": {},
   "source": [
    "## The Core Idea Behind LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d144c15",
   "metadata": {},
   "source": [
    "The key to LSTMs is the cell state, the horizontal line running through the top of the diagram.\n",
    "\n",
    "The cell state is kind of like a conveyor belt (Âø´ÈÄüÈÄöÈóú). It runs straight down the entire chain, with only some minor linear interactions (no non-linear interactions). It‚Äôs very easy for information to just flow along it unchanged.\n",
    "\n",
    "![](Pictures/LSTM3-C-line.png)\n",
    "\n",
    "The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates.\n",
    "\n",
    "Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.\n",
    "\n",
    "![](Pictures/LSTM3-gate.png)\n",
    "\n",
    "The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means ‚Äúlet nothing through,‚Äù while a value of one means ‚Äúlet everything through!‚Äù\n",
    "\n",
    "An LSTM has three of these gates, to protect and control the cell state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f883edd9",
   "metadata": {},
   "source": [
    "## Step-by-Step LSTM Walk Through\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ff5f2c",
   "metadata": {},
   "source": [
    "Step 1. Forget gate gets rid of irrelevant information\n",
    "\n",
    "The first step in our LSTM is to decide what information we‚Äôre going to throw away from the cell state. This decision is made by a sigmoid layer called the ‚Äúforget gate layer.‚Äù It looks at $h_{t‚àí1}$ and $x_t$, and outputs a number between 0 and 1 (this is what sigmoid function does) for each number in the cell state $C_{t‚àí1}$. A 1 ($f_t=1$) represents ‚Äúcompletely keep this‚Äù while a 0 ($f_t=0$) represents ‚Äúcompletely get rid of this.‚Äù\n",
    "\n",
    "![](Pictures/LSTM3-focus-f.png)\n",
    "\n",
    "Step 2. Store relevant information from current input\n",
    "\n",
    "The next step is to decide what new information we‚Äôre going to store in the cell state. This has two parts. First, a sigmoid layer called the ‚Äúinput gate layer‚Äù decides which values we‚Äôll update. Next, a tanh layer creates a vector of new candidate values, $\\tilde C_t$ (This is exactly what we will get from vanilla RNN), that could be added to the state. In the next step, we‚Äôll combine these two to create an update to the state.\n",
    "\n",
    "cf. Notice that $\\tilde C_t$ is exactly what we will get directly from vanilla RNN; However, in LSTM, it uses an term $i_t$ to adjust how much information we want to get updated from $\\tilde C_t$ \n",
    "\n",
    "![](Pictures/LSTM3-focus-i.png)\n",
    "\n",
    "Step 3. Selectively update cell state\n",
    "\n",
    "It‚Äôs now time to update the old cell state, $C_{t‚àí1}$, into the new cell state $C_t$. The previous steps already decided what to do, we just need to actually do it.\n",
    "\n",
    "We multiply the old state by $f_t$, forgetting the things we decided to forget earlier. Then we add $i_t*\\tilde C_t$. This is the new candidate values, scaled by how much we decided to update each state value.\n",
    "\n",
    "![](Pictures/LSTM3-focus-C.png)\n",
    "\n",
    "Step 4. Output gate returns a filtered version of the cell state\n",
    "\n",
    "Finally, we need to decide what we‚Äôre going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we‚Äôre going to output. Then, we put the cell state through tanh (to push the values to be between ‚àí1 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.\n",
    "\n",
    "![](Pictures/LSTM3-focus-o.png)\n",
    "\n",
    "\n",
    "In a nutshell, LSTM contains 3 gates (sigmoid layers): forget gate, input gate, and output gate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028144aa",
   "metadata": {},
   "source": [
    "## Variants on LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d7f117",
   "metadata": {},
   "source": [
    "What we‚Äôve described so far is a pretty normal LSTM. But not all LSTMs are the same as the above. In fact, it seems like almost every paper involving LSTMs uses a slightly different version. The differences are minor, but it‚Äôs worth mentioning some of them.\n",
    "\n",
    "*LSTM with Peephole Connections*\n",
    "\n",
    "One popular LSTM variant, introduced by **Gers & Schmidhuber (2000)**, is adding ‚Äúpeephole connections.‚Äù This means that we let the gate layers look at the cell state (ÂÅ∑ÁúãÂø´ÈÄüÈÄöÈóúÁöÑË≥áË®ä).\n",
    "\n",
    "![](Pictures/LSTM3-var-peepholes-1.png)\n",
    "\n",
    "The above diagram adds peepholes to all the gates, but many papers will give some peepholes and not others.\n",
    "\n",
    "*LSTM with Coupled Forget/Input Gates*\n",
    "\n",
    "Another variation is to use coupled forget and input gates. Instead of separately deciding what to forget and what we should add new information to, we make those decisions together (i.e., combining the forget gate and input gate together, so $i_t=1-f_t$). We only forget when we‚Äôre going to input something in its place. We only input new values to the state when we forget something older.\n",
    "\n",
    "![](Pictures/LSTM3-var-tied-1.png)\n",
    "\n",
    "\n",
    "*GRU*\n",
    "\n",
    "A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by **Cho, et al. (2014)**. It combines the forget and input gates into a single ‚Äúupdate gate.‚Äù It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models (less parameters and only two gates: update gate and reset gate), and has been growing increasingly popular.\n",
    "\n",
    "Notice that $r_t=0$: ignore previous memory and only stores the new information.\n",
    "\n",
    "\n",
    "![](Pictures/LSTM3-var-GRU-1.png)\n",
    "\n",
    "\n",
    "These are only a few of the most notable LSTM variants. There are lots of others, like Depth Gated RNNs by **Yao, et al. (2015)**. There‚Äôs also some completely different approach to tackling long-term dependencies, like Clockwork RNNs by **Koutnik, et al. (2014)**.\n",
    "\n",
    "Which of these variants is best? Do the differences matter? **Greff, et al. (2015)** do a nice comparison of popular variants, finding that they‚Äôre all about the same. **Jozefowicz, et al. (2015)** tested more than ten thousand RNN architectures, finding some that worked better than LSTMs on certain tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a291ac",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2292d343",
   "metadata": {},
   "source": [
    "GRU (Gated Recurrent Unit) aims to solve the vanishing gradient problem which comes with a standard recurrent neural network. GRU can also be considered as a variation on the LSTM because both are designed similarly and, in some cases, produce equally excellent results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3091646c",
   "metadata": {},
   "source": [
    "## How do GRUs work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfc229d",
   "metadata": {},
   "source": [
    "To solve the vanishing gradient problem of a standard RNN, GRU uses, so-called, **update gate** and **reset gate**. Basically, these are two vectors which decide what information should be passed to the output. The special thing about them is that they can be trained to keep information from long ago, without washing it through time or remove information which is irrelevant to the prediction.\n",
    "\n",
    "![](Pictures/LSTM3-var-GRU-1.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128b5589",
   "metadata": {},
   "source": [
    "## Update gate and Reset gate\n",
    "\n",
    "*Update gate*\n",
    "\n",
    "We start with calculating the update gate $z_t$ for time step $t$ using the formula:\n",
    "\n",
    "$$z_t=\\sigma(W_z\\cdot[h_{t-1}, x_t])$$\n",
    "\n",
    "The update gate helps the model to determine how much of the past information (from previous time steps) needs to be passed along to the future. That is really powerful because the model can decide to copy all the information from the past and eliminate the risk of vanishing gradient problem.\n",
    "\n",
    "*Reset gate*\n",
    "\n",
    "Essentially, this gate is used from the model to decide how much of the past information to forget. To calculate it, we use:\n",
    "\n",
    "$$r_t=\\sigma(W_r\\cdot[h_{t-1}, x_t])$$\n",
    "\n",
    "*Current memory content*\n",
    "\n",
    "\n",
    "Let‚Äôs see how exactly the gates will affect the final output. First, we start with the usage of the reset gate. We introduce a new memory content which will use the reset gate to store the relevant information from the past. It is calculated as follows:\n",
    "\n",
    "$$\\tilde h_t = tanh(W\\cdot [r_t*h_{t-1},x_t])$$\n",
    "\n",
    "Notice the element-wise production in $r_t*h_{t-1}$. This will determine what to remove from the previous time steps. \n",
    "\n",
    "*Final memory at current time step*\n",
    "\n",
    "As the last step, the network needs to calculate $h_t$ ‚Äî vector which holds information for the current unit and passes it down to the network. In order to do that the update gate is needed. It determines what to collect from the current memory content ‚Äî $\\tilde h_t$ and what from the previous steps ‚Äî $h_{t-1}$. That is done as follows:\n",
    "\n",
    "$$h_t = (1-z_t)* h_{t-1}+z_t\\tilde h_t$$\n",
    "\n",
    "\n",
    "Following through, you can see how $z_t$ is used to calculate $1-z_t$ and then combined with $\\tilde h_t$. $z_t$ is also used with $h_{t-1}$ in an element-wise multiplication. Finally, $h_t$ is a result of the summation of the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4ac066",
   "metadata": {},
   "source": [
    "# Stateful and Stateless LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac409b6",
   "metadata": {},
   "source": [
    "A key to understanding the difference between stateful and stateless LSTMs is ‚Äúwhen internal state is reset‚Äù.\n",
    "\n",
    "Stateless (default): In the stateless LSTM configuration, internal state is reset after each training batch or each batch when making predictions.\n",
    "\n",
    "Stateful: In the stateful LSTM configuration, internal state is only reset when the `reset_state()` function is called.\n",
    "\n",
    "When to Use `stateful=True`:\n",
    "\n",
    "1. Time-Series Data with Sequential Dependencies: If your data has a strong sequential dependency, and the model needs to capture information from one sequence to the next, setting `stateful=True` might be beneficial.\n",
    "\n",
    "2. Batch Processing with Sequential Data: In scenarios where you are processing data in batches, and the order of the sequences matters, using a stateful RNN can be appropriate.\n",
    "\n",
    "Considerations:\n",
    "\n",
    "1. Batch Size: When using a stateful RNN, it's crucial to set an appropriate batch size. The batch size determines how many sequences are processed in parallel, and it should align with the structure of your data.\n",
    "2. Careful Initialization: When using a stateful RNN, the initial state is important. You may need to explicitly manage and initialize the internal state of the model.\n",
    "3. Resetting State: If you want to reset the internal state (e.g., at the end of an epoch), you need to explicitly call `reset_states()` on the model.\n",
    "\n",
    "For more details, please check [Stateful and Stateless LSTM for Time Series Forecasting with Python](https://machinelearningmastery.com/stateful-stateless-lstm-time-series-forecasting-python/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f45ffd",
   "metadata": {},
   "source": [
    "# Shuffling with LSTM\n",
    "\n",
    "In general, when you shuffle the training data (a set of sequences), you shuffle the order in which sequences are fed to the RNN, you don't shuffle the ordering within individual sequences. This is fine to do **when your network is stateless**:\n",
    "\n",
    "Stateless Case:\n",
    "\n",
    "The network's memory only persists for the duration of a sequence. Training on sequence B before sequence A doesn't matter because the network's memory state does not persist across sequences.\n",
    "\n",
    "On the other hand:\n",
    "\n",
    "Stateful Case:\n",
    "\n",
    "The network's memory persists across sequences. Here, you cannot blindly shuffle your data and expect optimal results. Sequence A should be fed to the network before sequence B because A comes before B, and we want the network to evaluate sequence B with memory of what was in sequence A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0402985",
   "metadata": {},
   "source": [
    "# Stacked LSTM\n",
    "\n",
    "Stacking LSTM hidden layers makes the model deeper, more accurately earning the description as a deep learning technique.\n",
    "\n",
    "Stacked LSTMs or Deep LSTMs were introduced by **Speech Recognition with Deep Recurrent Neural Network (Graves, et al)**. in their application of LSTMs to speech recognition, beating a benchmark on a challenging standard problem.\n",
    "\n",
    "\n",
    "In the same work, they found that the depth of the network was more important than the number of memory cells in a given layer to model skill.\n",
    "\n",
    "Stacked LSTMs are now a stable technique for challenging sequence prediction problems. A Stacked LSTM architecture can be defined as an LSTM model comprised of multiple LSTM layers. An LSTM layer above provides a sequence output rather than a single value output to the LSTM layer below. Specifically, one output per input time step, rather than one output time step for all input time steps.\n",
    "\n",
    "![](Pictures/stacked_lstm.png)\n",
    "\n",
    "\n",
    "In practice, notice that each LSTMs memory cell requires a 3D input. When an LSTM processes one input sequence of time steps, each memory cell will output a single value for the whole sequence as a 2D array.\n",
    "\n",
    "To stack LSTM layers, we need to change the configuration of the prior LSTM layer to output a 3D array as input for the subsequent layer.\n",
    "\n",
    "We can do this by setting the `return_sequences` argument on the layer to `True` (defaults to False). This will return one output for each input time step and provide a 3D array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79918d9",
   "metadata": {},
   "source": [
    "# Overfitting Issue and Recurrent Dropout\n",
    "\n",
    "Dropout is one of the most popular regularization techniques in deep learning.\n",
    "\n",
    "At each training iteration, dropout randomly chooses different nodes to ignore.\n",
    "\n",
    "This prevents the network from relying on any single neuron.\n",
    "\n",
    "\n",
    "Recurrent Dropout: use drop out to fight overfitting in the recurrent layers (in addition to drop out for the dense layers)\n",
    "\n",
    "The **same recurrnt dropout pattern** should be applied at every timestep.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831cf58",
   "metadata": {},
   "source": [
    "# Miscellaneous LSTMs\n",
    "\n",
    "## Bi-directional LSTM\n",
    "\n",
    "## CNN-LSTM\n",
    "\n",
    "## Conv-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57547497",
   "metadata": {},
   "source": [
    "# Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd24ca4",
   "metadata": {},
   "source": [
    "## Making Predictions with Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340da5aa",
   "metadata": {},
   "source": [
    "The sequence imposes an explicit order on the observations.\n",
    "\n",
    "The order is important. It must be respected in the formulation of prediction problems that use the sequence data as input or output for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a0b2d0",
   "metadata": {},
   "source": [
    "### Sequence Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d6f7a5",
   "metadata": {},
   "source": [
    "Sequence prediction involves predicting the next value for a given input sequence.\n",
    "\n",
    "For example:\n",
    "\n",
    "- Given: 1, 2, 3, 4, 5\n",
    "- Predict: 6\n",
    "\n",
    "![](Pictures/sequence_pred.png)\n",
    "\n",
    "Some examples of sequence prediction problems include:\n",
    "\n",
    "\n",
    "- Stock Market Prediction. Given a sequence of movements of a security over time, predict the next movement of the security.\n",
    "- Product Recommendation. Given a sequence of past purchases of a customer, predict the next purchase of a customer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322ab7aa",
   "metadata": {},
   "source": [
    "### Sequence-to-Sequence Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f8d37c",
   "metadata": {},
   "source": [
    "Sequence-to-sequence prediction involves predicting an output sequence given an input sequence.\n",
    "\n",
    "For example:\n",
    "\n",
    "- Given: 1, 2, 3, 4, 5\n",
    "- Predict: 6, 7, 8, 9, 10\n",
    "\n",
    "![](Pictures/seq2seq.png)\n",
    "\n",
    "If the input and output sequences are a time series, then the problem may be referred to as multi-step time series forecasting.\n",
    "\n",
    "- Multi-Step Time Series Forecasting. Given a time series of observations, predict a sequence of observations for a range of future time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11c032a",
   "metadata": {},
   "source": [
    "### Cardinality from Timesteps (NOT features!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95e9bab",
   "metadata": {},
   "source": [
    "The cardinality of the sequence prediction models defined above refers to time steps, not features (e.g. univariate or multivariate sequences).\n",
    "\n",
    "A sequence may be comprised of single values, one for each time step.\n",
    "\n",
    "Alternately, a sequence could just as easily represent a vector of multiple observations at the time step. Each item in the vector for a time step may be thought of as its own separate time series. \n",
    "\n",
    "For example, a model that takes as **input one time step** of temperature and pressure and **predicts one time step** of temperature and pressure is a **one-to-one model**, not a many-to-many model.\n",
    "\n",
    "![](Pictures/Multiple-Feature.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cb6b42",
   "metadata": {},
   "source": [
    "# Data Preparation for LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54ea209",
   "metadata": {},
   "source": [
    "## Stationary Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eac26fb",
   "metadata": {},
   "source": [
    "Trends can result in a varying mean over time, whereas seasonality can result in a changing variance over time, both which define a time series as being non-stationary. Stationary datasets are those that have a stable mean and variance, and are in turn much easier to model.\n",
    "\n",
    "Differencing is a popular and widely used data transform for making time series data stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a138a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "\tdiff = list()\n",
    "\tfor i in range(interval, len(dataset)):\n",
    "\t\tvalue = dataset[i] - dataset[i - interval]\n",
    "\t\tdiff.append(value)\n",
    "\treturn diff\n",
    "\n",
    "# invert differenced forecast\n",
    "def inverse_difference(last_ob, value):\n",
    "\treturn value + last_ob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3753847c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n"
     ]
    }
   ],
   "source": [
    "# define a dataset with a linear trend\n",
    "data = [i+1 for i in range(20)]\n",
    "print(data)\n",
    "# difference the dataset\n",
    "diff = difference(data)\n",
    "print(diff)\n",
    "# invert the difference\n",
    "inverted = [inverse_difference(data[i], diff[i]) for i in range(len(diff))]\n",
    "print(inverted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b6d889",
   "metadata": {},
   "source": [
    "## Scaling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc03b1b",
   "metadata": {},
   "source": [
    "When a network is fit on unscaled data that has a range of values (e.g. quantities in the 10s to 100s) it is possible for large inputs to slow down the learning and convergence of your network and in some cases prevent the network from effectively learning your problem.\n",
    "\n",
    "### Normalize Series Data\n",
    "\n",
    "Normalization is a rescaling of the data from the original range so that all values are within the range of 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583f6306",
   "metadata": {},
   "source": [
    "```\n",
    "y = (x - min) / (max - min)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19f2003",
   "metadata": {},
   "source": [
    "You can normalize your dataset using the scikit-learn object `MinMaxScaler`.\n",
    "\n",
    "Good practice usage with the `MinMaxScaler` and other scaling techniques is as follows:\n",
    "\n",
    "- Fit the scaler using available **training data**. For normalization, this means the training data will be used to estimate the minimum and maximum observable values. This is done by calling the `fit()` function.\n",
    "- Apply the scale to **training data**. This means you can use the normalized data to train your model. This is done by calling the `transform()` function.\n",
    "- Apply the scale to data going forward. This means you can prepare new data in the future on which you want to make predictions.\n",
    "\n",
    "If needed, the transform can be inverted. This is useful for converting predictions back into their original scale for reporting or plotting. This can be done by calling the `inverse_transform()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11d3f84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     10.0\n",
      "1     20.0\n",
      "2     30.0\n",
      "3     40.0\n",
      "4     50.0\n",
      "5     60.0\n",
      "6     70.0\n",
      "7     80.0\n",
      "8     90.0\n",
      "9    100.0\n",
      "dtype: float64\n",
      "[[ 10.]\n",
      " [ 20.]\n",
      " [ 30.]\n",
      " [ 40.]\n",
      " [ 50.]\n",
      " [ 60.]\n",
      " [ 70.]\n",
      " [ 80.]\n",
      " [ 90.]\n",
      " [100.]]\n",
      "Min: 10.000000, Max: 100.000000\n",
      "[[0.        ]\n",
      " [0.11111111]\n",
      " [0.22222222]\n",
      " [0.33333333]\n",
      " [0.44444444]\n",
      " [0.55555556]\n",
      " [0.66666667]\n",
      " [0.77777778]\n",
      " [0.88888889]\n",
      " [1.        ]]\n",
      "[[ 10.]\n",
      " [ 20.]\n",
      " [ 30.]\n",
      " [ 40.]\n",
      " [ 50.]\n",
      " [ 60.]\n",
      " [ 70.]\n",
      " [ 80.]\n",
      " [ 90.]\n",
      " [100.]]\n"
     ]
    }
   ],
   "source": [
    "from pandas import Series\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# define contrived series\n",
    "data = [10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0]\n",
    "series = Series(data)\n",
    "print(series)\n",
    "# prepare data for normalization\n",
    "values = series.values\n",
    "values = values.reshape((len(values), 1))\n",
    "print(values)\n",
    "# train the normalization\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler = scaler.fit(values)\n",
    "print('Min: %f, Max: %f' % (scaler.data_min_, scaler.data_max_))\n",
    "# normalize the dataset and print\n",
    "normalized = scaler.transform(values)\n",
    "print(normalized)\n",
    "# inverse transform and print\n",
    "inversed = scaler.inverse_transform(normalized)\n",
    "print(inversed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d23c023",
   "metadata": {},
   "source": [
    "### Standardize Series Data\n",
    "\n",
    "Standardizing a dataset involves rescaling the distribution of values so that the mean of observed values is 0 and the standard deviation is 1.\n",
    "\n",
    "Standardization assumes that your observations fit a Gaussian distribution (bell curve) with a well behaved mean and standard deviation. You can still standardize your time series data if this expectation is not met, but you may not get reliable results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71233040",
   "metadata": {},
   "source": [
    "```\n",
    "y = (x - mean) / standard_deviation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a19867",
   "metadata": {},
   "source": [
    "You can standardize your dataset using the scikit-learn object `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27db78eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.0\n",
      "1    5.5\n",
      "2    9.0\n",
      "3    2.6\n",
      "4    8.8\n",
      "5    3.0\n",
      "6    4.1\n",
      "7    7.9\n",
      "8    6.3\n",
      "dtype: float64\n",
      "Mean: 5.355556, StandardDeviation: 2.712568\n",
      "[[-1.60569456]\n",
      " [ 0.05325007]\n",
      " [ 1.34354035]\n",
      " [-1.01584758]\n",
      " [ 1.26980948]\n",
      " [-0.86838584]\n",
      " [-0.46286604]\n",
      " [ 0.93802055]\n",
      " [ 0.34817357]]\n",
      "[[1. ]\n",
      " [5.5]\n",
      " [9. ]\n",
      " [2.6]\n",
      " [8.8]\n",
      " [3. ]\n",
      " [4.1]\n",
      " [7.9]\n",
      " [6.3]]\n"
     ]
    }
   ],
   "source": [
    "from pandas import Series\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import sqrt\n",
    "# define contrived series\n",
    "data = [1.0, 5.5, 9.0, 2.6, 8.8, 3.0, 4.1, 7.9, 6.3]\n",
    "series = Series(data)\n",
    "print(series)\n",
    "# prepare data for normalization\n",
    "values = series.values\n",
    "values = values.reshape((len(values), 1))\n",
    "# train the normalization\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(values)\n",
    "print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, sqrt(scaler.var_)))\n",
    "# normalize the dataset and print\n",
    "standardized = scaler.transform(values)\n",
    "print(standardized)\n",
    "# inverse transform and print\n",
    "inversed = scaler.inverse_transform(standardized)\n",
    "print(inversed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf279658",
   "metadata": {},
   "source": [
    "### Real-Valued Inputs\n",
    "\n",
    "You may have a sequence of quantities as inputs, such as **prices** or temperatures.\n",
    "\n",
    "If the distribution of the quantity is normal, then it should be standardized, otherwise the series should be normalized. This applies if the range of quantity values is large (10s 100s, etc.) or small (0.01, 0.0001).\n",
    "\n",
    "If the quantity values are small (near 0-1) and the distribution is limited (e.g. standard deviation near 1) then perhaps you can get away with no scaling of the series.\n",
    "\n",
    "### Scaling Output Variable\n",
    "\n",
    "You must ensure that the scale of your output variable matches the scale of the activation function (transfer function) on the output layer of your network.\n",
    "\n",
    "If the output is a real value, this is best modeled with a linear activation function. If the distribution of the value is normal, then you can standardize the output variable. Otherwise, the output variable can be normalized.\n",
    "\n",
    "### Considerations\n",
    "\n",
    "- Estimate descriptive statistics. You can estimate descriptive statistics (min and max values for normalization or mean and standard deviation for standardization) from the training data. Inspect these first-cut estimates and use domain knowledge or domain experts to help improve these estimates so that they will be usefully correct on all data in the future.\n",
    "- Save descriptive statistics. You will need to normalize new data in the future in exactly the same way as the data used to train your model. Save the descriptive statistics used to file and load them later when you need to scale new data when making predictions.\n",
    "- Data Analysis. Use data analysis to help you better understand your data. For example, a simple histogram can help you quickly get a feeling for the distribution of quantities to see if standardization would make sense.\n",
    "- Scale Each Series. If your problem has multiple series, treat each as a separate variable and in turn scale them separately.\n",
    "- Scale At The Right Time. It is important to apply any scaling transforms at the right time. For example, if you have a series of quantities that is non-stationary, it may be appropriate to **scale after first making your data stationary**. It would not be appropriate to scale the series after it has been transformed into a supervised learning problem as each column would be handled differently, which would be incorrect.\n",
    "- Scale if in Doubt. You probably do need to rescale your input and output variables. If in doubt, **at least normalize** your data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbcb973",
   "metadata": {},
   "source": [
    "## Handle Missing Timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6295c1",
   "metadata": {},
   "source": [
    "Sequences must be framed as a supervised learning problem when using neural networks.\n",
    "\n",
    "That means the sequence needs to be divided into input and output pairs.\n",
    "\n",
    "e.g. $y_t = f(X_t, X_{t-1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f400daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[       nan 0.68627597] => nan\n",
      "[0.68627597 0.74802302] => 0.686275974635361\n",
      "[0.74802302 0.66601443] => 0.7480230160283416\n",
      "[0.66601443 0.21218973] => 0.6660144337919112\n",
      "[0.21218973 0.32765495] => 0.21218972806821557\n",
      "[0.32765495 0.57127038] => 0.32765494520847505\n",
      "[0.57127038 0.61242966] => 0.571270382249948\n",
      "[0.61242966 0.38275331] => 0.6124296573244923\n",
      "[0.38275331 0.3765882 ] => 0.38275330826352305\n",
      "[0.3765882 0.9972823] => 0.3765882027159263\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "from numpy import array\n",
    "from pandas import concat\n",
    "from pandas import DataFrame\n",
    "\n",
    "# generate a sequence of random values\n",
    "def generate_sequence(n_timesteps):\n",
    "\treturn [random() for _ in range(n_timesteps)]\n",
    "\n",
    "# generate data for the lstm\n",
    "def generate_data(n_timesteps):\n",
    "\t# generate sequence\n",
    "\tsequence = generate_sequence(n_timesteps)\n",
    "\tsequence = array(sequence)\n",
    "\t# create lag\n",
    "\tdf = DataFrame(sequence)\n",
    "\tdf = concat([df.shift(1), df], axis=1)\n",
    "\tvalues = df.values\n",
    "\t# specify input and output data\n",
    "\tX, y = values, values[:, 0]\n",
    "\treturn X, y\n",
    "\n",
    "# generate sequence\n",
    "n_timesteps = 10\n",
    "X, y = generate_data(n_timesteps)\n",
    "# print sequence\n",
    "for i in range(n_timesteps):\n",
    "\tprint(X[i], '=>', y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301ce03b",
   "metadata": {},
   "source": [
    "We can generate sequences of random values between 0 and 1 using the `random()` function in the random module.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The Pandas `shift()` function can be used to create a shifted version of the sequence that can be used to represent the observations at the prior timestep. This can be concatenated with the raw sequence to provide the $X_{t-1}$ and $X_{t}$ input values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009ce8b3",
   "metadata": {},
   "source": [
    "### Remove all rows that contain a NaN value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c2270a",
   "metadata": {},
   "source": [
    "This can be done by `dropna()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1772294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data for the lstm\n",
    "def generate_data(n_timesteps):\n",
    " # generate sequence\n",
    " sequence = generate_sequence(n_timesteps)\n",
    " sequence = array(sequence)\n",
    " # create lag\n",
    " df = DataFrame(sequence)\n",
    " df = concat([df.shift(1), df], axis=1)\n",
    " # remove rows with missing values\n",
    " df.dropna(inplace=True)\n",
    " values = df.values\n",
    " # specify input and output data\n",
    " X, y = values, values[:, 0]\n",
    " return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b58a851",
   "metadata": {},
   "source": [
    "### Replace Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22f07d7",
   "metadata": {},
   "source": [
    "we can replace all NaN values with a specific value that does not appear naturally in the input, such as -1. To do this, we can use the `fillna()` Pandas function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca98d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data for the lstm\n",
    "def generate_data(n_timesteps):\n",
    " # generate sequence\n",
    " sequence = generate_sequence(n_timesteps)\n",
    " sequence = array(sequence)\n",
    " # create lag\n",
    " df = DataFrame(sequence)\n",
    " df = concat([df.shift(1), df], axis=1)\n",
    " # replace missing values with -1\n",
    " df.fillna(-1, inplace=True)\n",
    " values = df.values\n",
    " # specify input and output data\n",
    " X, y = values, values[:, 1]\n",
    " return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0994d713",
   "metadata": {},
   "source": [
    "### Masking Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06327fea",
   "metadata": {},
   "source": [
    "The marked missing input values can be masked from all calculations in the network.\n",
    "\n",
    "We can do this by using a `Masking layer` as the first layer to the network.\n",
    "\n",
    "When defining the layer, we can specify which value in the input to mask. If all features for a timestep contain the masked value, then the whole timestep will be excluded from calculations.\n",
    "\n",
    "This provides a middle ground between excluding the row completely and forcing the network to learn the impact of marked missing values.\n",
    "\n",
    "Because the Masking layer is the first in the network, it must specify the expected shape of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5141834b",
   "metadata": {},
   "source": [
    "# LSTM in Practice\n",
    "\n",
    "Please check how I implement LSTM with tensorflow on Google Colab for a better undestanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905b5bf3",
   "metadata": {},
   "source": [
    "## Define Network\n",
    "\n",
    "The first step is to define your network.\n",
    "\n",
    "Neural networks are defined in Keras as a sequence of layers. The container for these layers is the Sequential class.\n",
    "\n",
    "The first step is to create an instance of the Sequential class. Then you can create your layers and add them in the order that they should be connected. The LSTM recurrent layer comprised of memory units is called LSTM(). A fully connected layer that often follows LSTM layers and is used for outputting a prediction is called Dense()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afcac29",
   "metadata": {},
   "source": [
    "### Reshape Input layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d8dc36",
   "metadata": {},
   "source": [
    "The first layer in the network must define the number of inputs to expect. Input must be three-dimensional, comprised of samples, timesteps, and features.\n",
    "\n",
    "- Samples. These are the rows in your data. One sequence is one sample. A batch is comprised of one or more samples.\n",
    "- Timesteps. These are the past observations for a feature, such as lag variables. One time step in one point of observation in the sample\n",
    "- Features. These are columns in your data. One feature is one observation at a time step.\n",
    "\n",
    "![](Pictures/input_shape.png)\n",
    "\n",
    "\n",
    "When defining the input layer of your LSTM network, the network assumes you have 1 or more samples and requires that you specify the number of time steps and the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4caab0",
   "metadata": {},
   "source": [
    "### Windowing Data set (Rolling window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c92cb80",
   "metadata": {},
   "source": [
    "Windowing is a method to turn a time series dataset into supervised learning problem.\n",
    "\n",
    "![](Pictures/rolling_window.png)\n",
    "\n",
    "In other words, we want to use windows of the past to predict the future.\n",
    "\n",
    "For example for a univariate time series, windowing for one week (window=7) to predict the next single value (horizon=1) might look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487661d5",
   "metadata": {},
   "source": [
    "\n",
    "Window for one week (univariate time series)\n",
    "\n",
    "```\n",
    "[0, 1, 2, 3, 4, 5, 6] -> [7]\n",
    "[1, 2, 3, 4, 5, 6, 7] -> [8]\n",
    "[2, 3, 4, 5, 6, 7, 8] -> [9]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961f8dcb",
   "metadata": {},
   "source": [
    "- Window size (input): number of time steps of historical data used to predict horizon\n",
    "- Horizon (output): number of time steps to predict into the future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69d269c",
   "metadata": {},
   "source": [
    "### Choice of Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c797e3e3",
   "metadata": {},
   "source": [
    "The choice of activation function is most important for the output layer as it will define the format that predictions will take.\n",
    "\n",
    "For a common predictive modeling problem (e.g. **Regression**) types and the structure and standard activation function that you can use in the output layer:\n",
    "\n",
    "**Regression**: Linear activation function, or ‚Äòlinear‚Äô, and the number of neurons matching the number of outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843c806a",
   "metadata": {},
   "source": [
    "## Compile Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22a3560",
   "metadata": {},
   "source": [
    "Once we have defined our network, we must compile it.\n",
    "\n",
    "Compilation is an efficiency step. It transforms the simple sequence of layers that we defined into a highly efficient series of matrix transforms in a format intended to be executed on your GPU or CPU, depending on how Keras is configured.\n",
    "\n",
    "Think of compilation as a precompute step for your network. It is always required after defining a model.\n",
    "\n",
    "\n",
    "Compilation requires a number of parameters to be specified, specifically tailored to training your network. Specifically, the optimization algorithm to use to train the network and the loss function used to evaluate the network that is minimized by the optimization algorithm.\n",
    "\n",
    "For example, below is a case of compiling a defined model and specifying the **stochastic gradient descent (sgd)** optimization algorithm and the **mean squared error** loss function, intended for a **regression** type problem.\n",
    "\n",
    "Perhaps the most commonly used optimization algorithms because of their generally better performance are:\n",
    "\n",
    "- **Stochastic Gradient Descent**, that requires the tuning of a learning rate and momentum.\n",
    "- **ADAM**, that requires the tuning of learning rate.\n",
    "- **RMSprop**, that requires the tuning of learning rate.\n",
    "\n",
    "Finally, you can also specify **metrics** to collect while fitting your model in addition to the loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d8c57e",
   "metadata": {},
   "source": [
    "## Fit Network\n",
    "\n",
    "Once the network is compiled, it can be fit, which means adapt the weights on a training dataset.\n",
    "\n",
    "Fitting the network requires the training data to be specified, both a matrix of input patterns, X, and an array of matching output patterns, y.\n",
    "\n",
    "The network is trained using the backpropagation algorithm and optimized according to the optimization algorithm and loss function specified when compiling the model.\n",
    "\n",
    "The backpropagation algorithm requires that the network be trained for a specified number of **epochs** or exposures to the training dataset.\n",
    "\n",
    "Each epoch can be partitioned into groups of input-output pattern pairs called **batches**. This defines the number of patterns that the network is exposed to before the weights are updated within an epoch. It is also an efficiency optimization, ensuring that not too many input patterns are loaded into memory at a time.\n",
    "\n",
    "Once fit, a **history** object is returned that provides a summary of the performance of the model during training. This includes both the loss and any additional metrics specified when compiling the model, recorded each epoch.\n",
    "\n",
    "Training can take a long time, from seconds to hours to days depending on the size of the network and the size of the training data.\n",
    "\n",
    "By default, a progress bar is displayed on the command line for each epoch. This may create too much noise for you, or may cause problems for your environment, such as if you are in an interactive notebook or IDE.\n",
    "\n",
    "You can **reduce the amount of information displayed** to just the loss each epoch by setting the verbose argument to 2. You can turn off all output by setting `verbose=0`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123209af",
   "metadata": {},
   "source": [
    "## Evaluate Network\n",
    "\n",
    "Once the network is trained, it can be evaluated.\n",
    "\n",
    "The network can be evaluated on the training data, but this will not provide a useful indication of the performance of the network as a predictive model, as it has seen all of this data before.\n",
    "\n",
    "We can evaluate the performance of the network on a separate dataset, unseen during testing. This will provide an estimate of the performance of the network at making predictions for unseen data in the future.\n",
    "\n",
    "The model evaluates the loss across all of the test patterns, as well as any other metrics specified when the model was compiled. A list of evaluation metrics is returned.\n",
    "\n",
    "As with fitting the network, verbose output is provided to give an idea of the progress of evaluating the model. We can turn this off by setting the `verbose = 0`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b82bca",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "\n",
    "Once we are satisfied with the performance of our fit model, it's recommended to save the model and then we can use it to make predictions on new data.\n",
    "\n",
    "The model is saved in HDF5 (`h5`) file format that efficiently stores large arrays of numbers on disk. You will need to confirm that you have the h5py Python library installed. \n",
    "\n",
    "This is as easy as calling the predict() function on the model with an array of new input patterns.\n",
    "\n",
    "The predictions will be returned in the format provided by the output layer of the network.\n",
    "\n",
    "In the case of a **regression** problem, these predictions may be in the format of the problem directly, provided by a linear activation function.\n",
    "\n",
    "As with fitting and evaluating the network, verbose output is provided to given an idea of the progress of the model making predictions. We can turn this off by setting the `verbose = 0`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f079450",
   "metadata": {},
   "source": [
    "# Labs: LSTM\n",
    "\n",
    "\n",
    "\n",
    "Check on Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1463312",
   "metadata": {},
   "source": [
    "# Temporal Convolution Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1a7c9d",
   "metadata": {},
   "source": [
    "# N-BEATS Algorithm (Neural Basis Expansion Analysis For Interpretable Time Series Forecasting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc8bdba",
   "metadata": {},
   "source": [
    "N-beats is a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. \n",
    "\n",
    "## N-BEATS Architecture\n",
    "\n",
    "![](Pictures/N-Beats_arch.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a102187d",
   "metadata": {},
   "source": [
    "## Time Series Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105835f2",
   "metadata": {},
   "source": [
    "![](Pictures/ts_input.webp)\n",
    "\n",
    "\n",
    "The model takes time series data upto $x_t$ (t=data points upto time t) as its input and predict future $x_{t+h}$ (h = Forecast window length). The size of the input is $nH$ (n generally ranging from 2 to 7) also called the Lookback Period, model learn the behaviour of the time series over the lookback period and tries to predict the behaviour of data upto ‚Äú$H$ future points‚Äù also called Forecast Period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2cc18a",
   "metadata": {},
   "source": [
    "## Architecture Blocks:\n",
    "\n",
    "Time series data over the Lookback period serves as input to Stack 1, which in-turn is made up of multiple Blocks, which are arranged in a Doubly Residual Stacking manner. In order to understand working of the architecture we have to understand the Basic Block first.\n",
    "\n",
    "\n",
    "### Basic Block\n",
    "\n",
    "![Figure3](Pictures/block1.webp)\n",
    "\n",
    "\n",
    "![Figure4](Pictures/block2.webp)\n",
    "\n",
    "\n",
    "Input to Stack 1 will pass through a Block which looks exactly as represented in Figure 3, Figure 4 is the internal layers configuration of Basic Block for clear understanding.\n",
    "\n",
    "We have set length of Forecast period to 5 ($H = 5$) data points and Lookback Period to 15 ($nH=15$) data points.\n",
    "\n",
    "15 dimensional input from the Lookback Period is first passed through a 4 layer [FC+Relu] stack and afterwards divided into two parts. Each of which is further passed through another FC and finally we get two outputs, a 15 dim vector in the form of Backcast and a 5 dim vector in the form of Forecast. This Basic Block learns to predict not only the future data points in the form of Forecast but also predicts the input data as well in the form of Backcast.\n",
    "\n",
    "\n",
    "### Doubly Residual Stacking of Blocks\n",
    "\n",
    "![](Pictures/stack_struc.webp)\n",
    "\n",
    "A single Stack consists of multiple Basic Blocks, arranged in a manner following Double Residual Stacking principal. There are two arithmetic operations going on with the output of the Basic Block (i.e Backcast and Forecast) hence the term Double Residual Stacking.\n",
    "\n",
    "A 15 dim input of the Lookback Period ($\\text{Lookback_inp}$) is passed through Block 1 which gives us two outputs $\\text{Backcast}_1$ and $\\text{Forecast}_1$. The 15 dim input to Block 2 will be element wise subtraction of $\\text{Backcast}_1$ with $\\text{Lookback_inp}$( i.e., $\\text{Backcast}_1-\\text{Lookback_inp}$ ). By subtracting $\\text{Lookback_inp}$ from $\\text{Backcast}_1$, a vector which incorporates only those learnings not learned enough by Block 1 will be passed as input to Block 2.\n",
    "\n",
    "Following this logic, input to every Block would be a 15 dim vector which is made up of element wise subtraction of previous Block‚Äôs Backcast output and input. The Backcast output of the last Block in a Stack is called Stack Backcast output.\n",
    "\n",
    "Whereas the Forecast output of all the blocks in the Stack will be added element wise to yield a 5 dim vector. This vector is going to serve as Stack Forecast Output.\n",
    "\n",
    "### Combining the Stacks\n",
    "\n",
    "![](Pictures/stack_comb.webp)\n",
    "\n",
    "It is pretty evident from the above image that a 15 dim $\\text{Lookback_inp}$ vector when passed through Stack 1 will yield two outputs, a 15 dim Stack Backcast Output and a 5 dim Stack Forecast Output. Stack Backcast Output will serve as input to Stack 2 (this vector represents learnings not learnt by Stack 1) and similarly Stack Backcast Output from individual Stack will act as input to next Stack down the line. Stack Forecast Output from all the Stacks will be summed element wise together to yield the final 5-dim Global Forecast Vector.\n",
    "\n",
    "Loss using MSE(mean squared error) will be calculated on this predicted Global Forecast Vector and Ground Truth data.\n",
    "\n",
    "All the Gradients in the architecture will be updated based on this Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6af8e",
   "metadata": {},
   "source": [
    "## Learning Trend:\n",
    "\n",
    "\n",
    "![](Pictures/trend_block.webp)\n",
    "\n",
    "In order to learn trend from the lookback period, we remove the last layer of the Basic Block and multiply outputs X and Y with two matrices as mentioned in the above figure. This change is only incorporated in the Basic Block, author of the paper refers this new Block as Trend Block. These matrices are formed according to the logic mentioned in the N-beats paper.\n",
    "\n",
    "![](Pictures/trend_equ.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea901f0",
   "metadata": {},
   "source": [
    "## Learning Seasonality\n",
    "\n",
    "![](Pictures/seasonality_block.webp)\n",
    "\n",
    "In order to learn seasonality from the lookback period, we remove the last layer of Basic Block and multiply outputs X and Y with two matrices as mentioned in the above figure. This change is only incorporated in the Basic Block, author of the paper refer this new Block as Seasonality Block. These matrices are formed according to the logic mentioned in the N-beats paper.\n",
    "\n",
    "![](Pictures/seasonality_equ.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb29ff6",
   "metadata": {},
   "source": [
    "# Labs: N-Beats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b6165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "182.255432px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
